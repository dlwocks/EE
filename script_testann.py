# The following script is to help you determine the general speed of a particular ann implementation.
# This script does NOT test the accuracy of the implementation.
# import ann as an ann class that you want to test before this line (say, from cythonann import ann), and copy following line
#
# Note on how to profile: go to site-packages/line-profiler, and run py -3 kernprof.py -l [script file]
def checktime():
    from timeit import timeit
    setup = '''
    from __main__ import ann
    from numpy import array, sin, abs
    from random import uniform
    def target(inp):
        return abs(sin(4 * inp) / 4 / inp)
    inp = array([[(i+0.1)/10] for i in range(5)])
    ans = target(inp)
    '''
    code = '''
    a = ann([1, 4, 1])
    a.train(inp, ans)
    '''
    return timeit(code,setup=setup, number=100)

# Commit 177557785f620ef51d7671e9434d5af6d63ed999: cythonann takes 20.s
# Commit 75fc33f841e125eb6260f316746889c50ae2ab88: 3.s


####################################################################################

# The following script is to help you determine whether a particular ann implementation is bahaving correctly.
# import ann as an ann class that you want to test before this line (say, from cythonann import ann), and copy following line

def naivetest():
    import numpy as np
    from learningfunc import ann
    from numpy import sin, abs, linspace, arange, array
    def target(inp):
        return abs(sin(inp) / inp)

    inp = linspace(-10, 10, 1000)[None].T
    ans = target(inp)

    a = ann([1, 20, 1])
    minres = a.train(inp, ans)  # This probably needs a few minute
    print('fun:', minres.fun)
    x = linspace(-10, 10, 1000)
    import matplotlib.pyplot as plt
    plt.plot(x, target(x))
    plt.plot(x, [a.get(array([i])) for i in x])
    plt.show()

from numpy import sin, abs, array
def target(inp):
    return abs(sin(inp) / inp)

# Or more conveniently using function call:
def test(target=target, hiddenlayer=[10], plot=True, rng=(-10,10), density=50, verbose=False):
    '''
    Test the accuracy of ANN implemented in learningfunc.py
    Arguments:
        target: a function take 1d array as input and output 1d array of same length as output.
                Target that the ann trains toward.
                If not inputted, would use `abs(sin(inp) / inp)` where `inp` is input.
                Note that implemented ANN would output from 0 to 1, so if target function returns value out of that boundary, the test would not work
        hiddenlayer: a list specifying the hidden layer in the ann trained.
        plot: whether to plot the target function and the trained ann's output.
        rng: A length-2 tuple represents upper and lower bound of input to target function and to ANN
             Inclusive.
        density: A real number. There would be `round((rng[1] - rng[0]) * density)` training example
                 generated by target function
        verbose: True or False.
    Output:
        A float representing the difference between the actual error and
        the minimium error that can be possibly reached
        (When the ANN output is exactly same as the target output).
    '''
    from numpy import linspace
    from learningfunc import ann
    num = round((rng[1] - rng[0]) * density)
    if target is None:
        if 'target' in globals():
            target = globals()['target']
        else:
            raise RuntimeError('No target function in global variable or in argument')
    inp = linspace(rng[0], rng[1], num)[None].T
    ans = target(inp)
    if verbose:
        print('Target input and output is generated')
    if len(ans) != num:
        raise RuntimeError('target function did not return array of same length with input.')
    import matplotlib.pyplot as plt
    def mincost(ans):
        from numpy import log
        return (ans * -log(ans) - (1 - ans) * log(1 - ans)).sum() / len(ans)
    a = ann([1] + hiddenlayer + [1])
    if verbose:
        print('Start training ANN..')
    minres = a.train(inp, ans)
    if plot:
        plt.plot(inp, target(inp))
        plt.plot(inp, [a.get(array([i])) for i in inp])
        plt.show()
    fun = minres.fun
    minfun = mincost(ans)
    if verbose:
        print('Actual Error is: %d\nMinimun achievable Error is %d' % (fun, minfun))
    return fun - minfun

# Find out the least possible cost for the given answer:
def mincost(ans):
    from numpy import log
    (ans * -log(ans) - (1 - ans) * log(1 - ans)).sum()

# Test the relation of hidden layer to convergence of ann, using the above test function
def testing_test():
    from statistics import mean
    import numpy as np
    ls = []
    for i in range(1, 51):
        c = min([test(hiddenlayer=[i], plot=False) for t in range(3)])
        ls.append(c)
        print('%d hidden layer tested' % i)

    import matplotlib.pyplot as plt
    x = np.arange(1, 51)
    plt.plot(x, ls)
    plt.show()
